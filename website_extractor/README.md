# Interactive Website Content Extractor

A user-friendly tool for extracting and managing web content locally. This tool is particularly useful for research, documentation, and content management in the FactoryXChange project.

## Features

- ğŸ–¥ï¸ Interactive menu-driven interface
- ğŸŒ Multiple extraction modes:
  - Single website extraction
  - Batch processing of multiple URLs
  - Domain crawling with depth control
  - Subdomain discovery and extraction
- ğŸ“‚ Organized content storage
- ğŸ¯ Automatic browser preview
- ğŸ’¾ Local file support
- ğŸ–¼ï¸ Asset preservation (CSS, JS, images)
- ğŸ”— Link management

## Installation

1. Make sure you have Python 3.6 or higher installed
2. Install the required dependencies:
   ```bash
   cd Tools/website_extractor
   pip install -r requirements.txt
   ```

## Quick Start

Run the interactive tool:
```bash
python interactive_extract.py
```

## Usage Guide

### 1. Single Website Extraction
Perfect for grabbing individual reference pages or documentation.

```
1. Select option 1 from the menu
2. Enter URL (or local file path)
3. Type 'done'
4. Content opens automatically in your browser
```

Example URLs:
- Web URL: `https://example.com/skills-framework`
- Local file: `../html5up-stellar/global-skills-taxonomy.html`

### 2. Batch Processing
Ideal for collecting multiple reference materials at once.

```
1. Select option 2 from the menu
2. Paste multiple URLs (one per line or space-separated)
3. Type 'done'
4. Choose which extracted site to view
```

Example batch input:
```
weforum.org/reports/future-of-jobs-2023
digitalsme.eu/skills-taxonomy
lightcast.io/open-skills
done
```

### 3. Domain Crawling
Useful for comprehensive content extraction from a single source.

```
1. Select option 3 from the menu
2. Enter the root URL
3. Specify depth limits (optional)
4. Type 'done'
```

Crawling options:
- Max pages: Control total pages extracted
- Max depth: Limit how deep the crawler goes
- Leave blank for unlimited

### 4. Subdomain Extraction
Great for exploring different sections of a website.

```
1. Select option 4 from the menu
2. Enter the main domain
3. Type 'done'
```

Common subdomains checked:
- www
- blog
- docs
- api
- support
- help

## Output Structure

```
extracted_sites/
â””â”€â”€ project_name/
    â”œâ”€â”€ index.html          # Main content
    â””â”€â”€ assets/
        â”œâ”€â”€ css/           # Styling
        â”œâ”€â”€ js/            # Scripts
        â””â”€â”€ images/        # Media files
```

For batch operations:
```
extracted_sites/
â”œâ”€â”€ project_name_1/
â”œâ”€â”€ project_name_2/
â””â”€â”€ project_name_3/
```

## Tips & Tricks

1. **Local Files**: You can extract local HTML files to create organized copies with all assets
2. **Batch Processing**: Copy-paste multiple URLs at once to save time
3. **Browser Preview**: Extracted content automatically opens in your default browser
4. **Depth Control**: Use max_depth for focused crawling of specific sections
5. **Asset Management**: All resources are downloaded and organized automatically

## Common Use Cases

1. **Research Collection**
   - Extract multiple research papers
   - Save reference materials for offline access
   - Create local copies of documentation

2. **Content Management**
   - Organize web content into structured folders
   - Create offline backups of online resources
   - Prepare content for sharing or archiving

3. **Documentation**
   - Extract technical documentation
   - Save skills frameworks and taxonomies
   - Create local copies of project guidelines

## Troubleshooting

1. **URL Issues**
   - Make sure URLs include http:// or https://
   - Tool automatically adds https:// if missing

2. **Local Files**
   - Use relative paths: `../folder/file.html`
   - Or absolute paths: `C:/path/to/file.html`

3. **Asset Downloads**
   - Some sites may block automated downloads
   - Dynamic content might not be captured
   - Check console for any warning messages

## Notes

- The tool respects robots.txt
- Some websites may block automated access
- JavaScript-generated content may not be captured
- Use depth limits for large sites
- Local files work best with relative paths

## Support

For issues or suggestions:
- Check the error messages in the console
- Ensure all dependencies are installed
- Verify URL accessibility
- Check file permissions for local files 